{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42000 images belonging to 10 classes.\n",
      "Found 18000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img \n",
    "\n",
    "# reading and preprocessing images\n",
    "datagen = ImageDataGenerator(validation_split=0.3, rescale=1./255)\n",
    "datagen_test = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 256\n",
    "img_size = 28\n",
    "classes = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "train_it = datagen.flow_from_directory('/Users/ayoungkim/Desktop/Pattern Recognition and Machine Learning/Assignement1/train/', batch_size=batch_size, \n",
    "                                       target_size=(img_size, img_size), subset='training', \n",
    "                                       class_mode='sparse', color_mode='grayscale', classes=classes)\n",
    "val_it = datagen.flow_from_directory('/Users/ayoungkim/Desktop/Pattern Recognition and Machine Learning/Assignement1/train/', batch_size=batch_size, target_size=(img_size, img_size), \n",
    "                                    subset='validation', class_mode='sparse', color_mode='grayscale', \n",
    "                                     classes=classes)\n",
    "test_it = datagen_test.flow_from_directory('/Users/ayoungkim/Desktop/Pattern Recognition and Machine Learning/Assignement1/test/', class_mode=None, batch_size=batch_size, \n",
    "                                      target_size=(img_size, img_size), shuffle=False, color_mode='grayscale')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "total_classes = 10\n",
    "conv_layers = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(img_size,img_size, 1)))\n",
    "for i in range(conv_layers):\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(total_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 85,770\n",
      "Trainable params: 85,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "165/165 [==============================] - 70s 424ms/step - loss: 1.9179 - accuracy: 0.3018 - val_loss: 1.4539 - val_accuracy: 0.4641\n",
      "Epoch 2/60\n",
      "165/165 [==============================] - 63s 382ms/step - loss: 1.2696 - accuracy: 0.5292 - val_loss: 0.8425 - val_accuracy: 0.7286\n",
      "Epoch 3/60\n",
      "165/165 [==============================] - 60s 361ms/step - loss: 0.9531 - accuracy: 0.6461 - val_loss: 0.7509 - val_accuracy: 0.7291\n",
      "Epoch 4/60\n",
      "165/165 [==============================] - 60s 363ms/step - loss: 0.7511 - accuracy: 0.7257 - val_loss: 0.5727 - val_accuracy: 0.7911\n",
      "Epoch 5/60\n",
      "165/165 [==============================] - 61s 367ms/step - loss: 0.5933 - accuracy: 0.7844 - val_loss: 0.4023 - val_accuracy: 0.8708\n",
      "Epoch 6/60\n",
      "165/165 [==============================] - 61s 369ms/step - loss: 0.4753 - accuracy: 0.8286 - val_loss: 0.3427 - val_accuracy: 0.8806\n",
      "Epoch 7/60\n",
      "165/165 [==============================] - 58s 353ms/step - loss: 0.3851 - accuracy: 0.8620 - val_loss: 0.3860 - val_accuracy: 0.8536\n",
      "Epoch 8/60\n",
      "165/165 [==============================] - 59s 360ms/step - loss: 0.3213 - accuracy: 0.8882 - val_loss: 0.2954 - val_accuracy: 0.8946\n",
      "Epoch 9/60\n",
      "165/165 [==============================] - 67s 408ms/step - loss: 0.2632 - accuracy: 0.9084 - val_loss: 0.2001 - val_accuracy: 0.9287\n",
      "Epoch 10/60\n",
      "165/165 [==============================] - 65s 396ms/step - loss: 0.2203 - accuracy: 0.9216 - val_loss: 0.1592 - val_accuracy: 0.9456\n",
      "Epoch 11/60\n",
      "165/165 [==============================] - 67s 406ms/step - loss: 0.1851 - accuracy: 0.9355 - val_loss: 0.2251 - val_accuracy: 0.9176\n",
      "Epoch 12/60\n",
      "165/165 [==============================] - 64s 388ms/step - loss: 0.1622 - accuracy: 0.9438 - val_loss: 0.1228 - val_accuracy: 0.9559\n",
      "Epoch 13/60\n",
      "165/165 [==============================] - 62s 373ms/step - loss: 0.1342 - accuracy: 0.9533 - val_loss: 0.1178 - val_accuracy: 0.9593\n",
      "Epoch 14/60\n",
      "165/165 [==============================] - 63s 384ms/step - loss: 0.1163 - accuracy: 0.9599 - val_loss: 0.0781 - val_accuracy: 0.9737\n",
      "Epoch 15/60\n",
      "165/165 [==============================] - 65s 394ms/step - loss: 0.1008 - accuracy: 0.9652 - val_loss: 0.1144 - val_accuracy: 0.9582\n",
      "Epoch 16/60\n",
      "165/165 [==============================] - 72s 434ms/step - loss: 0.0884 - accuracy: 0.9699 - val_loss: 0.1080 - val_accuracy: 0.9621\n",
      "Epoch 17/60\n",
      "165/165 [==============================] - 94s 567ms/step - loss: 0.0799 - accuracy: 0.9739 - val_loss: 0.0638 - val_accuracy: 0.9779\n",
      "Epoch 18/60\n",
      "165/165 [==============================] - 89s 540ms/step - loss: 0.0711 - accuracy: 0.9752 - val_loss: 0.1367 - val_accuracy: 0.9534\n",
      "Epoch 19/60\n",
      "165/165 [==============================] - 86s 520ms/step - loss: 0.0589 - accuracy: 0.9795 - val_loss: 0.0554 - val_accuracy: 0.9811\n",
      "Epoch 20/60\n",
      "165/165 [==============================] - 88s 534ms/step - loss: 0.0580 - accuracy: 0.9800 - val_loss: 0.0711 - val_accuracy: 0.9761\n",
      "Epoch 21/60\n",
      "165/165 [==============================] - 82s 498ms/step - loss: 0.0497 - accuracy: 0.9830 - val_loss: 0.0543 - val_accuracy: 0.9811\n",
      "Epoch 22/60\n",
      "165/165 [==============================] - 71s 430ms/step - loss: 0.0461 - accuracy: 0.9840 - val_loss: 0.0544 - val_accuracy: 0.9818\n",
      "Epoch 23/60\n",
      "165/165 [==============================] - 82s 498ms/step - loss: 0.0429 - accuracy: 0.9856 - val_loss: 0.0607 - val_accuracy: 0.9801\n",
      "Epoch 24/60\n",
      "165/165 [==============================] - 93s 566ms/step - loss: 0.0369 - accuracy: 0.9875 - val_loss: 0.0465 - val_accuracy: 0.9846\n",
      "Epoch 25/60\n",
      "165/165 [==============================] - 86s 521ms/step - loss: 0.0353 - accuracy: 0.9886 - val_loss: 0.0447 - val_accuracy: 0.9856\n",
      "Epoch 26/60\n",
      "165/165 [==============================] - 91s 550ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 0.0436 - val_accuracy: 0.9856\n",
      "Epoch 27/60\n",
      "165/165 [==============================] - 87s 525ms/step - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.0477 - val_accuracy: 0.9843\n",
      "Epoch 28/60\n",
      "165/165 [==============================] - 80s 488ms/step - loss: 0.0265 - accuracy: 0.9906 - val_loss: 0.0510 - val_accuracy: 0.9832\n",
      "Epoch 29/60\n",
      "165/165 [==============================] - 76s 461ms/step - loss: 0.0232 - accuracy: 0.9924 - val_loss: 0.0462 - val_accuracy: 0.9861\n",
      "Epoch 30/60\n",
      "165/165 [==============================] - 81s 489ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.0522 - val_accuracy: 0.9836\n",
      "Epoch 31/60\n",
      "165/165 [==============================] - 68s 415ms/step - loss: 0.0220 - accuracy: 0.9921 - val_loss: 0.0588 - val_accuracy: 0.9821\n",
      "Epoch 32/60\n",
      "165/165 [==============================] - 81s 491ms/step - loss: 0.0191 - accuracy: 0.9934 - val_loss: 0.0492 - val_accuracy: 0.9856\n",
      "Epoch 33/60\n",
      "165/165 [==============================] - 88s 532ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.0406 - val_accuracy: 0.9881\n",
      "Epoch 34/60\n",
      "165/165 [==============================] - 91s 550ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.0450 - val_accuracy: 0.9869\n",
      "Epoch 35/60\n",
      "165/165 [==============================] - 92s 555ms/step - loss: 0.0159 - accuracy: 0.9944 - val_loss: 0.0539 - val_accuracy: 0.9833\n",
      "Epoch 36/60\n",
      "165/165 [==============================] - 91s 553ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.0409 - val_accuracy: 0.9878\n",
      "Epoch 37/60\n",
      "165/165 [==============================] - 86s 521ms/step - loss: 0.0144 - accuracy: 0.9951 - val_loss: 0.0454 - val_accuracy: 0.9863\n",
      "Epoch 38/60\n",
      "165/165 [==============================] - 90s 545ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.0421 - val_accuracy: 0.9889\n",
      "Epoch 39/60\n",
      "165/165 [==============================] - 87s 527ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0456 - val_accuracy: 0.9878\n",
      "Epoch 40/60\n",
      "165/165 [==============================] - 65s 394ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0638 - val_accuracy: 0.9829\n",
      "Epoch 41/60\n",
      "165/165 [==============================] - 79s 476ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0399 - val_accuracy: 0.9891\n",
      "Epoch 42/60\n",
      "165/165 [==============================] - 67s 409ms/step - loss: 0.0120 - accuracy: 0.9957 - val_loss: 0.0450 - val_accuracy: 0.9876\n",
      "Epoch 43/60\n",
      "165/165 [==============================] - 64s 387ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 0.0545 - val_accuracy: 0.9844\n",
      "Epoch 44/60\n",
      "165/165 [==============================] - 66s 397ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0526 - val_accuracy: 0.9868\n",
      "Epoch 45/60\n",
      "165/165 [==============================] - 63s 381ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.0682 - val_accuracy: 0.9824\n",
      "Epoch 46/60\n",
      "165/165 [==============================] - 69s 418ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.0552 - val_accuracy: 0.9859\n",
      "Epoch 47/60\n",
      "165/165 [==============================] - 66s 401ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0423 - val_accuracy: 0.9889\n",
      "Epoch 48/60\n",
      "165/165 [==============================] - 70s 427ms/step - loss: 0.0085 - accuracy: 0.9967 - val_loss: 0.0489 - val_accuracy: 0.9879\n",
      "Epoch 49/60\n",
      "165/165 [==============================] - 66s 403ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0460 - val_accuracy: 0.9887\n",
      "Epoch 50/60\n",
      "165/165 [==============================] - 66s 402ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0520 - val_accuracy: 0.9869\n",
      "Epoch 51/60\n",
      "165/165 [==============================] - 63s 384ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0660 - val_accuracy: 0.9837\n",
      "Epoch 52/60\n",
      "165/165 [==============================] - 66s 403ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0744 - val_accuracy: 0.9833\n",
      "Epoch 53/60\n",
      "165/165 [==============================] - 65s 392ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.0583 - val_accuracy: 0.9869\n",
      "Epoch 54/60\n",
      "165/165 [==============================] - 63s 383ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0500 - val_accuracy: 0.9886\n",
      "Epoch 55/60\n",
      "165/165 [==============================] - 69s 417ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0471 - val_accuracy: 0.9894\n",
      "Epoch 56/60\n",
      "165/165 [==============================] - 62s 378ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0614 - val_accuracy: 0.9859\n",
      "Epoch 57/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 67s 404ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0560 - val_accuracy: 0.9877\n",
      "Epoch 58/60\n",
      "165/165 [==============================] - 77s 470ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0441 - val_accuracy: 0.9901\n",
      "Epoch 59/60\n",
      "165/165 [==============================] - 61s 370ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0415 - val_accuracy: 0.9907\n",
      "Epoch 60/60\n",
      "165/165 [==============================] - 59s 357ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0527 - val_accuracy: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcbd3c41a00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        train_it,\n",
    "        epochs=60,\n",
    "        validation_data=val_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 images belonging to 1 classes.\n",
      "1: 45\n",
      "Found 6000 images belonging to 1 classes.\n",
      "2: 4\n",
      "Found 6000 images belonging to 1 classes.\n",
      "3: 9\n",
      "Found 6000 images belonging to 1 classes.\n",
      "4: 40\n",
      "Found 6000 images belonging to 1 classes.\n",
      "5: 17\n",
      "Found 6000 images belonging to 1 classes.\n",
      "6: 27\n",
      "Found 6000 images belonging to 1 classes.\n",
      "7: 29\n",
      "Found 6000 images belonging to 1 classes.\n",
      "8: 3\n",
      "Found 6000 images belonging to 1 classes.\n",
      "9: 30\n",
      "Found 6000 images belonging to 1 classes.\n",
      "10: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "\n",
    "# predict \n",
    "def prediction_file(images, ids, output_path):\n",
    "    prediction_logits = model.predict(images)\n",
    "    prediction = np.argmax(prediction_logits, axis=1) + 1 # classes: 1-10\n",
    "    \n",
    "    rows = [[id_file, yhat] for id_file, yhat in zip(ids, list(prediction))]\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(['Id', 'Category'])\n",
    "        writer.writerows(rows)\n",
    "    \n",
    "\n",
    "test_paths = sorted([path.stem for path in pathlib.Path('/Users/ayoungkim/Desktop/Pattern Recognition and Machine Learning/Assignement1/test/').glob('*test/*.jpg')])\n",
    "# Make csv file first and copy the path to output_path\n",
    "prediction_file(test_it, test_paths, '/Users/ayoungkim/Desktop/increased_epoch.csv')\n",
    "\n",
    "for i in range(10):\n",
    "    example_paths = sorted([path.stem for path in pathlib.Path('/Users/ayoungkim/Desktop/Pattern Recognition and Machine Learning/Assignement1/train/').glob('*'+str(i+1)+'/*.jpg')])\n",
    "    example_it = datagen_test.flow_from_directory('/Users/ayoungkim/Desktop/Pattern Recognition and Machine Learning/Assignement1/train/', batch_size=batch_size, \n",
    "                                              target_size=(img_size, img_size),\n",
    "                                          class_mode='sparse', color_mode='grayscale', \n",
    "                                          shuffle=False, classes=[str(i+1)])\n",
    "    prediction_logits = model.predict(example_it)\n",
    "    prediction = np.argmax(prediction_logits, axis=1) + 1 # classes: 1-10\n",
    "    print(str(i+1) + ': ' + str(np.count_nonzero(prediction != (i+1))))\n",
    "    \n",
    "    # prediction_file(example_it, example_paths, 'exe_'+str(i+1)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
